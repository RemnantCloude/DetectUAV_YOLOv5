#!/usr/bin/python
#-*- coding:utf-8 -*-
#
#@author:Cloude Remnant
#@date:2021-07-27
#@description:
#

import rospy
import numpy as np
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import torch
import os
from os.path import realpath, dirname, join
import glob

from models.experimental import attempt_load
from utils.general import check_img_size, non_max_suppression, scale_coords
from utils.plots import colors, plot_one_box
from utils.torch_utils import select_device, time_sync
from utils.augmentations import letterbox
from siamRPN.net import SiamRPNvot
from siamRPN.run_SiamRPN import SiamRPN_init, SiamRPN_track

from yolov5.msg import uav_position

target = {
    "siamRPN_init": False,
    "yolov5_detect": False,
    "yolov5_xyxy": [],
    "diff_times": 0
}

position_data = {"is_detacted": False, "position": [], "image_shape": []}


class OPT():
    def __init__(self):
        self.yolov5_weights = rospy.get_param("/yolov5_node/yolov5_weights",
                                              default="../yolov5s_old.pt")
        self.siamRPN_weights = rospy.get_param("/yolov5_node/siamRPN_weights",
                                               default="../SiamRPNVOT.model")

        self.imgsz = rospy.get_param("/yolov5_node/imgsz", default=640)
        self.conf_thres = rospy.get_param(
            "/yolov5_node/conf_thres",
            default=0.25)  #'object confidence threshold'
        self.iou_thres = rospy.get_param(
            "/yolov5_node/iou_thres", default=0.45)  #'IOU threshold for NMS'
        self.max_det = rospy.get_param(
            "/yolov5_node/max_det",
            default=1000)  #'maximum number of detections per image'
        self.device = rospy.get_param("/yolov5_node/device", default='')
        self.classes = rospy.get_param("/yolov5_node/classes", default=None)
        self.agnostic_nms = rospy.get_param(
            "/yolov5_node/agnostic_nms", default=True)  #'class-agnostic NMS'
        self.augment = rospy.get_param("/yolov5_node/augment",
                                       default=True)  #'augmented inference'
        self.line_thickness = rospy.get_param("/yolov5_node/line_thickness",
                                              default=3)
        self.half = rospy.get_param("/yolov5_node/half", default=False)

        self.siamRPN_conf_thres = rospy.get_param(
            "/yolov5_node/siamRPN_conf_thres", default=0.7)
        self.diff_times = rospy.get_param("/yolov5_node/diff_times", default=5)

        self.camera_field_angle_pitch = rospy.get_param(
            "/yolov5_node/camera_field_angle_pitch", default=180)  # 相机俯仰角
        self.camera_field_angle_yaw = rospy.get_param(
            "/yolov5_node/camera_field_angle_yaw", default=180)  # 相机方位角
        self.uav_size_x = rospy.get_param("/yolov5_node/uav_size_x", default=0)
        self.uav_size_y = rospy.get_param("/yolov5_node/uav_size_y", default=0)

        self.img_source = rospy.get_param("/yolov5_node/img_source", default=0)
        self.local_img_path = rospy.get_param("/yolov5_node/local_img_path",
                                              default="")
        self.read_img_interval = rospy.get_param(
            "/yolov5_node/read_img_interval", default=0.2)
        self.debug = rospy.get_param("/yolov5_node/debug", default=False)
        self.view_img = rospy.get_param("/yolov5_node/view_img", default=False)
        self.save_img = rospy.get_param("/yolov5_node/save_img", default=False)
        self.save_img_interval = rospy.get_param(
            "/yolov5_node/save_img_interval", default=0.2)
        self.save_img_path = rospy.get_param("/yolov5_node/save_img_path",
                                             default="")
        self.track = rospy.get_param("/yolov5_node/track", default=False)


class image():
    def __init__(self):
        # pointgray相机 # FIXME
        # 畸变系数
        k1, k2, p1, p2, k3 = 0.0030, -0.0032, 0.0018, 0.0025, -0.0006
        # 相机坐标系到像素坐标系的转换矩阵
        self.k = np.array([[339.9101, -0.2465, 682.4559],
                           [0, 339.0175, 487.1288], [0, 0, 1]])
        self.d = np.array([k1, k2, p1, p2, k3])
        self.save_img_lasttime = time_sync()
        self.save_img_interval = opt.save_img_interval
        self.save_img_cout = 0
        self.save_img_path = opt.save_img_path
        self.bridge = CvBridge()

    def __undistort(self, frame):
        h, w = frame.shape[:2]
        mapx, mapy = cv2.initUndistortRectifyMap(self.k, self.d, None, self.k,
                                                 (w, h), 5)
        return cv2.remap(frame, mapx, mapy, cv2.INTER_LINEAR)

    def convertImg(self, frame):
        frame = self.bridge.imgmsg_to_cv2(frame, "bgr8")
        img = cv2.flip(self.__undistort(frame), -1)
        return img

    def saveImg(self, frame):
        current_time = time_sync()
        if current_time - self.save_img_lasttime > self.save_img_interval:
            self.save_img_lasttime = current_time
            name = "{}.jpg".format(str(self.save_img_cout).rjust(6, "0"))
            if not os.path.exists(self.save_img_path):
                os.mkdir(self.save_img_path)
            cv2.imwrite(self.save_img_path + name, frame)
            rospy.loginfo("img-{} saved".format(self.save_img_cout))
            self.save_img_cout = self.save_img_cout + 1


def iou(box1, box2):
    """
    box1, box2 = [x1,y1,x2,y2] - top-left, bottom-right
    Returns: 
        float from 0 to 1
    """
    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

    iou_x1 = np.maximum(box1[0], box2[0])
    iou_y1 = np.maximum(box1[1], box2[1])
    iou_x2 = np.minimum(box1[2], box2[2])
    iou_y2 = np.minimum(box1[3], box2[3])

    area_iou = (iou_x2 - iou_x1) * (iou_y2 - iou_y1)
    iou = area_iou / (area_box1 + area_box2 - area_iou)

    return iou


def xyxy2xywh(x):
    xy = np.array([int((x[0] + x[2]) / 2), int((x[1] + x[3]) / 2)])
    wh = np.array([int(x[2] - x[0]), int(x[3] - x[1])])
    return xy, wh


def xywh2xyxy(xy, wh):
    x1 = int(xy[0] - wh[0] / 2)
    y1 = int(xy[1] - wh[1] / 2)
    x2 = int(xy[0] + wh[0] / 2)
    y2 = int(xy[1] + wh[1] / 2)
    return [x1, y1, x2, y2]


def positionPublish(position_data):
    """
    x = [x1,y1,x2,y2] - top-left, bottom-right
    img_size = [w,h]
    """
    global opt
    global uav_position_pub

    xy, wh = xyxy2xywh(position_data["position"])
    img_size = position_data["image_shape"]

    message = uav_position()
    message.is_detected = position_data["is_detacted"]
    if (message.is_detected == True):
        # 根据无人机所占像素解算距离信息
        message.distance_x = opt.uav_size_x * xy[0] / wh[0]
        message.distance_y = opt.uav_size_y * xy[1] / wh[1]
        # 鱼眼相机模型-等距投影
        message.angle_yaw = opt.camera_field_angle_yaw / 2 - (
            opt.camera_field_angle_yaw) * (xy[0] / img_size[0])
        message.angle_pitch = opt.camera_field_angle_pitch / 2 - (
            opt.camera_field_angle_pitch) * (xy[1] / img_size[1])

    uav_position_pub.publish(message)


def detect(img):
    global opt
    global target
    global position_data

    if opt.debug == True:
        t1 = time_sync()

    im0 = img.copy()
    img = letterbox(img, imgsz, stride)[0]
    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB
    img = np.ascontiguousarray(img)
    img = torch.from_numpy(img).to(opt.device)
    img = img.half() if opt.half else img.float()  # uint8 to fp16/32
    img /= 255.0  # 0 - 255 to 0.0 - 1.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    # Inference
    pred = model(img, augment=opt.augment, visualize=False)[0]

    # Apply NMS
    pred = non_max_suppression(pred,
                               opt.conf_thres,
                               opt.iou_thres,
                               opt.classes,
                               opt.agnostic_nms,
                               max_det=opt.max_det)

    # Process detections
    for det in pred:  # detections per image
        if len(det):
            # Rescale boxes from img_size to im0 size
            det[:, :4] = scale_coords(img.shape[2:], det[:, :4],
                                      im0.shape).round()

            max_conf = 0
            for item in det:  # 找到置信度最大的物体
                if item[-2] > max_conf:
                    max_conf_item = item
                    max_conf = max_conf_item[-2]

            xyxy = max_conf_item[:4]
            conf = max_conf_item[-2]
            cls = max_conf_item[-1]

            if opt.track == True:
                target["yolov5_detect"] = True
                target["yolov5_xyxy"] = xyxy.cpu().numpy()
            else:  # 只检测，不跟踪
                position_data["is_detacted"] = True
                position_data["position"] = xyxy.cpu().numpy().tolist()

            if opt.view_img:  # Add bbox to image
                c = int(cls)  # integer class
                label = 'drone {:.2f}'.format(conf)
                plot_one_box(xyxy,
                             im0,
                             label=label,
                             color=colors(c, True),
                             line_thickness=opt.line_thickness)
        elif opt.track == True:
            target["yolov5_detect"] = False
            target["diff_times"] = 0

    # Stream results
    if opt.view_img:
        cv2.imshow("detect", im0)
        key = cv2.waitKey(1)
        if key & 0xFF == ord('q'):
            rospy.on_shutdown()  # BUG

    if opt.debug == True:
        t2 = time_sync()
        rospy.loginfo("Detect time: %f", (t2 - t1))


def track(img):
    global opt
    global state
    global siam_net
    global target

    if opt.debug == True:
        t1 = time_sync()

    im0 = img.copy()
    xyxy = [0, 0, 0, 0]

    # initialize
    if target["siamRPN_init"] == False and target["yolov5_detect"] == True:
        target_pos, target_sz = xyxy2xywh(target["yolov5_xyxy"])
        state = SiamRPN_init(im0, target_pos, target_sz, siam_net)
        target["siamRPN_init"] = True
    # tracking
    if target["siamRPN_init"] == True:
        state = SiamRPN_track(state, im0)
        xyxy = xywh2xyxy(state["target_pos"], state["target_sz"])
        if opt.view_img == True:
            cv2.rectangle(im0, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]),
                          (0, 255, 255), opt.line_thickness)
        # compare with yolov5
        if target["yolov5_detect"] == True:
            if iou(target["yolov5_xyxy"],
                   xyxy) < opt.siamRPN_conf_thres:  # track the wrong target
                target["diff_times"] += 1
            position_data["is_detacted"] = True
            position_data["position"] = target[
                "yolov5_xyxy"]  # use detect target
        else:  # FIXME yolov5丢失目标后，可能完全跟踪到错误的目标
            position_data["is_detacted"] = True
            position_data["position"] = xyxy  # use track target

    # target loss, init net again
    if target["diff_times"] > opt.diff_times:
        target["diff_times"] = 0
        target["siamRPN_init"] = False

    if opt.view_img == True:
        cv2.imshow("track", im0)
        key = cv2.waitKey(1)
        if key & 0xFF == ord('q'):
            rospy.on_shutdown()  # BUG

    if opt.debug == True:
        t2 = time_sync()
        rospy.loginfo("Track time: %f", (t2 - t1))


def trackCallback(image_file):
    global my_image
    global opt
    global position_data

    if opt.img_source == 0:
        image_file = my_image.convertImg(image_file)

    if opt.save_img == True:
        my_image.saveImg(image_file)

    position_data["is_detacted"] = False
    position_data["image_shape"] = image_file.shape
    detect(image_file)

    if opt.track == True:
        track(image_file)

    positionPublish(position_data)


if __name__ == '__main__':
    opt = OPT()
    my_image = image()

    rospy.init_node('yolov5_node', anonymous=False)
    uav_position_pub = rospy.Publisher("/yolov5/uav_position",
                                       uav_position,
                                       queue_size=1)

    ## load yolov5 model
    # Initialize
    device = select_device(opt.device)
    opt.half &= device.type != 'cpu'  # half precision only supported on CUDA
    # Load model
    model = attempt_load(join(realpath(dirname(__file__)), opt.yolov5_weights),
                         map_location=device)  # load FP32 model
    rospy.loginfo("yolov5 Network successfully loaded")
    stride = int(model.stride.max())  # model stride
    imgsz = check_img_size(opt.imgsz, s=stride)  # check image size
    if opt.half == True:
        model.half()  # to FP16
    # Run inference
    if device.type != 'cpu':
        model(
            torch.zeros(1, 3, int(imgsz), int(imgsz)).to(device).type_as(
                next(model.parameters())))  # run once
    ## end

    if opt.track == True:
        # load net
        siam_net = SiamRPNvot()
        siam_net.load_state_dict(
            torch.load(join(realpath(dirname(__file__)), opt.siamRPN_weights)))
        rospy.loginfo("SiamRPNVOT Network successfully loaded")
        siam_net.eval().cuda()
        state = dict()

    # 0: external camera 1: local camera 2: local image dir
    if opt.img_source == 0:
        image_sub = rospy.Subscriber("/camera/image_color", Image,
                                     trackCallback)
        # image_sub = rospy.Subscriber("/camera/image_raw", Image,
        #                              trackCallback)
        rospy.spin()
    if opt.img_source == 1:
        cap = cv2.VideoCapture(0)
        while cap.isOpened():
            ret, frame = cap.read()
            if ret:
                trackCallback(frame)
    if opt.img_source == 2:
        image_files = sorted(glob.glob(opt.local_img_path))
        for image_file in image_files:
            frame = cv2.imread(image_file)
            trackCallback(frame)
            rospy.sleep(opt.read_img_interval)
        rospy.on_shutdown()  # BUG
